
# 1. [Conceptos introductorios de miner√≠a de datos](https://materials.campus.uoc.edu/cdocent/PID_00246834/)

## 1.1. ¬øQu√© es la miner√≠a de datos?

La **miner√≠a de datos (*Data Mining*)** es el proceso de **extraer conocimiento √∫til y no trivial a partir de grandes vol√∫menes de datos**, mediante el uso de t√©cnicas estad√≠sticas, matem√°ticas y de aprendizaje autom√°tico.

El objetivo principal es **transformar datos en informaci√≥n y conocimiento** para la **toma de decisiones**.

## 1.2. Etapas de un proyecto de miner√≠a de datos: **Metodolog√≠a CRISP-DM (*Cross Industry Standard Process for Data Mining*)**

La metodolog√≠a **CRISP-DM** (propuesta por DaimlerChrysler y SPSS) es el est√°ndar del sector. Propone un proceso **iterativo y basado en calidad total** (ciclo PDCA: _Plan‚ÄìDo‚ÄìCheck‚ÄìAct_).  
Un proyecto exitoso requiere planificaci√≥n, iteraci√≥n y revisi√≥n continua, evitando concentrar esfuerzos solo en el despliegue final.

Las seis fases del proceso son:

|Fase|Descripci√≥n|
|---|---|
|**1. Comprensi√≥n del negocio**|Definir los objetivos del proyecto desde la perspectiva empresarial.|
|**2. Comprensi√≥n de los datos**|Recopilar, describir y explorar los datos iniciales para detectar problemas de calidad o patrones preliminares.|
|**3. Preparaci√≥n de los datos**|Limpiar, transformar y seleccionar las variables relevantes para el modelado.|
|**4. Modelado**|Aplicar m√©todos estad√≠sticos o de _machine learning_ para crear modelos predictivos o descriptivos.|
|**5. Evaluaci√≥n**|Validar los modelos, analizar su rendimiento y comprobar si cumplen los objetivos de negocio.|
|**6. Despliegue**|Implementar el modelo en producci√≥n y generar reportes o sistemas autom√°ticos de decisi√≥n.|

> üîπ Las fases no son estrictamente secuenciales: pueden requerir iteraciones y revisiones.

![[Fases CRIPS-DM.png]]
### **1.2.1. Comprensi√≥n del negocio**

Se definen los **objetivos del negocio** y se eval√∫a la situaci√≥n actual, recursos y limitaciones. Se concretan los objetivos espec√≠ficos de miner√≠a de datos y se elabora un **plan de proyecto** con fases y tareas.

> Resultado: objetivos claros, plan estructurado.

### **1.2.2. Comprensi√≥n del negocio**

Busca familiarizarse con los datos: origen, calidad, estructura y problemas. Se analizan sus propiedades, se exploran patrones y se gestiona la calidad (limpieza y resoluci√≥n de inconsistencias).

> Resultado: diagn√≥stico completo de la ‚Äúmateria prima‚Äù.

### **1.2.3. Preparaci√≥n de los datos**

Se construye el **dataset final** para modelar, integrando datos relevantes, limpios y consistentes. Se documenta el proceso y se aplican t√©cnicas de **muestreo, selecci√≥n de atributos** o **reducci√≥n de dimensionalidad**.

> Resultado: conjunto de datos listo para modelado.

### **1.2.4. Modelado**

Se seleccionan las **t√©cnicas adecuadas** (√°rboles, redes neuronales, SVM, k-NN, clustering, etc.) seg√∫n el problema (clasificaci√≥n, regresi√≥n o segmentaci√≥n).  
Esta fase itera con la preparaci√≥n de datos y la evaluaci√≥n del modelo, ajustando par√°metros para mejorar la calidad.

> Resultado: modelo ajustado y validado t√©cnicamente.

### **1.2.5. Evaluaci√≥n del modelo**

Se analiza si el modelo cumple los **objetivos de negocio** y si existen descubrimientos adicionales relevantes. Los resultados deben verificarse en entornos de prueba antes del despliegue.

> Resultado: ranking de modelos, validaci√≥n y posibles l√≠neas futuras.

### **1.2.6. Despliegue**

Se implementan los modelos y se planifica su **mantenimiento y seguimiento**. Se define c√≥mo difundir el conocimiento obtenido y c√≥mo medir los beneficios del despliegue.

> Resultado: modelo operativo y conocimiento transferido.

---
# 2. [Tipolog√≠a de tareas y m√©todos en procesos de miner√≠a de datos](https://materials.campus.uoc.edu/cdocent/PID_00251983/)

## **2.1. Tipos de tareas**

Seg√∫n el objetivo de nuestro an√°lisis, podemos distinguir entre tres grandes
grupos de tareas, que revisaremos brevemente a continuaci√≥n.
### **2.1.1. Clasificaci√≥n**

La **clasificaci√≥n** consiste en asignar cada instancia a una **clase predefinida** o categor√≠a.  
El atributo objetivo (**variable dependiente**) es **discreto o categ√≥rico**.

**Ejemplo:**  
Predecir si un correo electr√≥nico es _spam_ o _no spam_.

**Formalizaci√≥n matem√°tica:**

$$
f: X \rightarrow Y  
$$
donde:

- ( X ) es el conjunto de **variables de entrada** o _features_,
- ( Y = {C<sub>1</sub>, C<sub>2</sub>, ..., C<sub>k</sub>} ) es el **conjunto finito de clases**,
- ( f ) es la **funci√≥n de clasificaci√≥n** que asigna cada instancia ( x<sub>i</sub> ) a una clase ( C<sub>j</sub> ).

**Ejemplo visual:**  
Un modelo que aprende a separar puntos rojos (clase 1) y azules (clase 2) en funci√≥n de sus coordenadas.

![[Clasificacion.png]]

### **2.1.2. Regresi√≥n**

En la **regresi√≥n**, la variable objetivo es **continua o num√©rica**.  
El objetivo es estimar una funci√≥n que relacione los predictores con un valor real.

**Ejemplo:**  
Predecir el **precio de una vivienda** seg√∫n su tama√±o y ubicaci√≥n.

**Modelo lineal general:**

$$
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \varepsilon_i  
$$

donde:

- ( y<sub>i</sub> ): valor real de salida,
- ( x<sub>ij</sub> ): variables predictoras,
- (beta): coeficientes del modelo,
- (varepsilon): t√©rmino de error.

![[Regresion.png]]

### **2.2.3. Agrupamiento o *clustering***

El **agrupamiento** o _clustering_ busca **descubrir estructuras ocultas** en los datos **sin conocer etiquetas previas**.  

El modelo debe determinar **cu√°ntos grupos existen** y **qu√© instancias pertenecen a cada uno**.

**Ejemplo:**  
Segmentar clientes seg√∫n sus h√°bitos de consumo sin conocer previamente los grupos.

**Formalizaci√≥n:**

$$
\text{Asignar } n \text{ instancias } {x_1, ..., x_n} \text{ a } k \text{ clusters } {C_1, ..., C_k}  
$$  
minimizando una funci√≥n de disimilitud:

$$
J = \sum_{i=1}^k \sum_{x_j \in C_i} ||x_j - \mu_i||^2  
$$

donde ($\mu$<sub>i</sub> ) es el centroide del cluster (i).

![[clustering.png]]

## **2.2. Tipos de m√©todos o algoritmos**

Respecto a la tipolog√≠a de m√©todos o algoritmos, discernimos entre dos grandes grupos:
### **2.2.1. Supervisados**

Los **m√©todos supervisados** requieren **datos etiquetados**, es decir, cada instancia tiene una clase o valor objetivo conocido.  
Incluyen:

- **Clasificaci√≥n** ‚Üí atributo objetivo **categ√≥rico**
- **Regresi√≥n** ‚Üí atributo objetivo **num√©rico**

**Ejemplos:**  
√Årboles de decisi√≥n, regresi√≥n lineal, _Support Vector Machines (SVM)_, redes neuronales.

![[Aprendizaje supervisado.png|500]]
### **2.2.2. No supervisados**

En los **m√©todos no supervisados**, el conjunto de datos **no tiene etiquetas**.  
Buscan estructuras internas o patrones de similitud.

![[Aprendizaje no supervisado.png|500]]

**Subtipos:**
- **Jer√°rquicos:**
    - Construyen una **estructura arborescente (dendrograma)**.
    - Estrategias _bottom-up_ (aglomerativas) o _top-down_ (divisivas).

- **Particionales:**
    - Dividen el conjunto de datos en **k grupos disjuntos**, sin jerarqu√≠a.
    - Ejemplo: _k-means_.

![[Tipologias de metodos de agrupamiento.png]]

Una misma t√©cnica puede resolver distintos tipos de problemas.

> **Tabla resumen:** √Årboles de decisi√≥n, redes neuronales, SVM, k-means, m√©todos probabil√≠sticos y k-NN pueden aplicarse en diferentes combinaciones de tareas supervisadas o no supervisadas.


---

#  3. [Datos de entrenamiento y test en miner√≠a de datos](https://materials.campus.uoc.edu/cdocent/PID_00251987/)

## **3.1. Conjuntos entrenamiento y test**

Sirven para construir y evaluar modelos, evitando el **sobreentrenamiento (overfitting)**.  
El conjunto de **entrenamiento** entrena el modelo; el de **test** eval√∫a su capacidad de generalizaci√≥n sobre datos nunca vistos.  

> Es esencial que ambos conjuntos sean **disjuntos**.

![[proceso de entrenamiento-test.png]]

Habitualmente, los juegos de datos de entrenamiento y de test suelen ser extracciones
aleatorias del juego de datos inicial. En funci√≥n del n√∫mero de
datos disponibles, existen diferentes t√©cnicas para la construcci√≥n de los conjuntos
de entrenamiento y de prueba.
## **3.2. Generaci√≥n de conjuntos de entrenamiento y test**

### **3.2.1. _Leave-One-Out (LOO)_**

Para un conjunto de ( n ) instancias:
- Se entrena con ( n-1 ) y se eval√∫a con 1.
- Se repite ( n ) veces.
- El error final es la media de los ( n ) errores.

$$  
E = \frac{1}{n} \sum_{i=1}^{n} e_i  
$$
> √ötil con pocos datos.
### **3.2.2. _Leave-p-Out (LpO)_**

- Similar al anterior, pero dejando ( p ) instancias para test.
- Computacionalmente costoso para grandes ( n ).

#### 3. _k-Fold Cross Validation_

- Divide el conjunto en ( k ) particiones disjuntas.
- Se entrena con ( k-1 ) y se prueba con la restante.
- Se repite ( k ) veces.

$$
E = \frac{1}{k} \sum_{i=1}^{k} e_i  
$$

> Valores comunes: ( k = 5 ) o ( k = 10 ).  
> Cuando ( k = n ), equivale a _Leave-One-Out_.

![[K-fold cross validation.png]]

Dividir los datos es fundamental para **evaluar la capacidad de generalizaci√≥n del modelo** y **evitar el sobreentrenamiento**.

## **3.3. *Overfitting* o sobreentrenamiento**

El **sobreentrenamiento (overfitting)** ocurre cuando un modelo se ajusta **demasiado** a los datos de entrenamiento, capturando **ruido o variabilidad aleatoria**.

![[overfitting.png]]

Esto provoca un bajo error de entrenamiento, pero un alto error en nuevos datos.

**Diagn√≥stico:**
- Error de entrenamiento ‚Üì
- Error de test ‚Üë

**Soluciones comunes:**
- Aumentar los datos de entrenamiento
- Regularizaci√≥n
- Validaci√≥n cruzada
- _Early stopping_ (detener el entrenamiento antes del sobreajuste)

En la gr√°fica de la izquierda se observa c√≥mo el error de entrenamiento disminuye progresivamente con el n√∫mero de iteraciones, lo cual es un comportamiento esperado en cualquier proceso de aprendizaje. Sin embargo, en la gr√°fica de la derecha, que muestra la precisi√≥n sobre el conjunto de prueba, se aprecia que a partir de la iteraci√≥n 50 el modelo deja de mejorar: la precisi√≥n se estabiliza y oscila en torno al 70-80 %.

Esto indica que, aunque el modelo sigue reduciendo el error en los datos de entrenamiento, no est√° mejorando su capacidad para predecir datos nuevos. En otras palabras, est√° **sobreajust√°ndose**: aprende demasiado bien los patrones espec√≠ficos del conjunto de entrenamiento, pero pierde capacidad de **generalizaci√≥n**. Esta situaci√≥n, conocida como **sobreentrenamiento**, es algo que debemos evitar para obtener modelos realmente √∫tiles y robustos.

![[evaluacion sobremuestreo.png]]

## 3.4. Conjunto de validaci√≥n

El **conjunto de validaci√≥n** es un tercer subconjunto usado para **ajustar hiperpar√°metros** o **comparar modelos**.

**Proceso t√≠pico:**
1. Entrenar con el conjunto de entrenamiento.
2. Evaluar distintas configuraciones sobre el conjunto de validaci√≥n.
3. Escoger el modelo √≥ptimo.
4. Medir el rendimiento final con el conjunto de test.

> Se recomienda usar tres conjuntos distintos: entrenamiento, validaci√≥n y test, garantizando independencia para evaluar la **capacidad de generalizaci√≥n**.

---

# 4. [Evaluaci√≥n de resultados en procesos de miner√≠a de datos](https://materials.campus.uoc.edu/cdocent/PID_00251985/)

La evaluaci√≥n depende del tipo de problema: **clasificaci√≥n**, **regresi√≥n** o **agrupamiento**.

El objetivo es cuantificar el grado o valor de ¬´bondad¬ª de la soluci√≥n encontrada, permitiendo la comparaci√≥n entre distintos m√©todos sobre los mismos conjuntos de datos.
## **4.1. Modelos de clasificaci√≥n**

Se eval√∫an mediante **matrices de confusi√≥n** y m√©tricas derivadas:
### **4.1.1. Matriz de confusi√≥n**

La **matriz de confusi√≥n** resume las predicciones del modelo comparando clases reales y predichas.


|                         | Predicci√≥n positiva     | Predicci√≥n negativa     |
| ----------------------- | ----------------------- | ----------------------- |
| **Clase positiva real** | Verdadero Positivo (VP) | Falso Negativo (FN)     |
| **Clase negativa real** | Falso Positivo (FP)     | Verdadero Negativo (VN) |

A partir de la matriz de confusi√≥n, definimos un conjunto de m√©tricas que permiten cuantificar la bondad de un modelo de clasificaci√≥n:

$$
\text{Exactitud (Accuracy)} = \frac{VP + VN}{VP + VN + FP + FN}  
$$

$$
\text{Precisi√≥n (Precision)} = \frac{VP}{VP + FP}  
$$

$$
\text{Exhaustividad (Recall)} = \frac{VP}{VP + FN}  
$$

$$
F1 = 2 \cdot \frac{\text{Precisi√≥n} \cdot \text{Recall}}{\text{Precisi√≥n} + \text{Recall}}  
$$


**Interpretaci√≥n:**
- _Accuracy_: porcentaje total de aciertos.
- _Precision_: fiabilidad de las predicciones positivas.
- _Recall_: capacidad de detectar verdaderos positivos.
- _F1_: media arm√≥nica entre precisi√≥n y recall.

 En algunos problemas nos interesar√°n o el error de clasificaci√≥n o exactitud general del modelo ,sino evitar alg√∫n tipo de errores espec√≠ficos.
 
**Ejemplo:**
- Supongamos que nuestro modelo est√° intentando predecir si existe la presencia de tumor o c√°ncer a partir de unos datos de entrenamiento. Lo que nos interesa, y el objetivo del modelo, es que no se den los casos de falsos negativos, en los cuales una instancia que deber√≠a considerarse positiva, ha sido dada como negativa. El caso contrario, aunque no nos interesa, no es tan grave. Si un paciente que no tiene c√°ncer, el sistema lo da como s√≠, un segundo an√°lisis lo descartar√≠a. Lo que nos interesa es maximizar este subgrupo en el cual  no haya ning√∫n caso positivo de tumor que el sistema descarte y d√© como negativo. En estos casos se usa la tasa de verdaderos positivos o tasa de verdaderos negativos.

### **4.1.2. Curvas ROC (Receiver Operating Characteristic)**

La **curva ROC** (_Receiver Operating Characteristic_) representa la relaci√≥n entre:
- Eje X: tasa de falsos positivos (TFP)
- Eje Y: tasa de verdaderos positivos (TVP)

A partir de la tasa de falsos positivos (TFP) y verdaderos positivos (TVP) ,se construyen las curvas ROC. Estas curvas permiten el c√°lculo del √°rea bajo esta curva, que nos da un valor de bondad o de optimizaci√≥n del modelo en general.

![[curva roc.png]]

La situaci√≥n ideal de un modelo, vemos la figura izquierda, es cuando los verdaderos positivos alcanzan el nivel m√°ximo mientras que los falsos positivos el nivel m√≠nimo. Esta es una situaci√≥n ideal a la que no siempre se puede llegar. En la figura del medio vemos una curva en donde el √°rea est√° alrededor del 0.8. Esta es una situaci√≥n m√°s habitual, m√°s real. La figura de la derecha presenta un ejemplo en donde esta √°rea, que corresponde a la mitad, equivale a un modelo aleatorio. Si nuestro modelo se acerca a esta situaci√≥n, estamos en un modelo de bajo entrenamiento y no estamos aportando mucho m√°s que un modelo aleatorio de generaci√≥n.

El **√Årea Bajo la Curva (AUC)** mide el rendimiento general:
- ( AUC = 1 ) ‚Üí modelo perfecto
- ( AUC = 0.5 ) ‚Üí modelo aleatorio

A modo de gu√≠a para interpretar las curvas ROC se han establecido los siguientes
intervalos para los valores de AUC:
- [0,5,0,6): Test malo
- [0,6,0,75): Test regular
- [0,75,0,9): Test bueno
- [0,9,0,97): Test muy bueno
- [0,97,1): Test excelente

## **4.2. Modelos de regresi√≥n**

En el caso de los modelos de regresi√≥n, b√°sicamente, usamos un conjunto, distintas m√©tricas, que nos permitan comparar el valor obtenido con el valor de la clase, el valor del conjunto de datos etiquetado.

Para problemas con variables continuas, se eval√∫a la **diferencia entre valores reales y predichos**.

| M√©trica                                    | F√≥rmula                                                                 | Interpretaci√≥n                                     |
| ------------------------------------------ | ----------------------------------------------------------------------- | -------------------------------------------------- |
| **Error Absoluto Medio (MAE)**             | MAE = $\frac{1}{n}\sum_{i=1}^{n}$                                       | $y_i - \hat{y}_i$                                  |
| **Error Cuadr√°tico Medio (MSE)**           | MSE = $\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$                    | Penaliza m√°s los errores grandes                   |
| **Ra√≠z del Error Cuadr√°tico Medio (RMSE)** | RMSE = $\sqrt{MSE}$                                                     | Error medio en las mismas unidades que la variable |
| **Coeficiente de determinaci√≥n (R¬≤)**      | $R^2$ = $1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}$ ) | Proporci√≥n de la varianza explicada                |

## **4.3. Modelos de agrupamiento**

Dado que no existen etiquetas, se usan **m√©tricas de calidad**, evaluando el rendimiento seg√∫n la **cohesi√≥n interna** y la **separaci√≥n entre grupos**.

### **4.3.1. M√©tricas de calidad por partici√≥n**

Por un lado est√°n las m√©tricas de calidad por partici√≥n, que eval√∫an, dentro de cada partici√≥n, la calidad esperada. Por ejemplo, una de las m√°s b√°sicas ser√≠a el di√°metro. B√°sicamente, mide la distancia m√°xima entre dos instancias de una misma partici√≥n.

- **Di√°metro (D):**  compactaci√≥n interna.
$$
D(C_i) = \max_{x, y \in C_i} d(x, y)  
$$
Claramente, las particiones compactas, es decir, con un di√°metro peque√±o,
suelen ser las m√°s deseadas. Cuando aparecen particiones con di√°metros muy
dispares, puede ser un signo de que debemos ampliar (o reducir) el n√∫mero de
particiones.

> Cuanto menor, mayor cohesi√≥n.

En la figura de la derecha vemos la distancia m√°xima que hay entre los dos puntos rojos o verdes. Esto ser√≠a el di√°metro. Cuanto menor sea este valor, nos indica mayor cohesi√≥n en estas particiones, lo cual es, l√≥gicamente, deseable.

![[distancias.png|300]]

Otra medida interesante es la separaci√≥n, que mide la m√≠nima distancia entre dos partici√≥n eso dos clusters. En este caso, nos interesa maximizar esta medida, porque nos indica que las dos particiones est√°n separadas entre ellas.

- **Separaci√≥n (S):**  distancia entre grupos.
$$  
S(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y)  
$$  
Cuanto mayor, mejor separaci√≥n entre grupos.

### 4.3.2. Calidad general

A partir de aqu√≠ hay otro nivel, otra categor√≠a: las m√©tricas de calidad general. En lugar de darnos un valor para cada partici√≥n, nos dan un valor general para todo el modelo, lo cual es deseable porque es m√°s f√°cil la comparaci√≥n entre distintos modelos. Aqu√≠ podemos destacar el √≠ndice Dunn el √≠ndice C.

- **√çndice Dunn:**  compara m√≠nima separaci√≥n y m√°xima dispersi√≥n (valores altos = buena calidad).
$$
D = \frac{\min_{i \ne j} S(C_i, C_j)}{\max_k D(C_k)}  
$$ 
Valores altos indican mejor particionado.

### **4.3.3. Calidad externa**

Finalmente, hay otro tipo de medidas de calidad, que llamamos de calidad externa, que aunque estemos en un modelo no supervisado, donde no tenemos datos etiquetados, donde no tenemos variables objetivo o clase, suponemos que de alguna forma podemos obtener esta informaci√≥n externamente y podemos obtener una posible clasificaci√≥n que luego comparamos con el resultado de nuestro modelo. Y definimos algo similar a la matriz de confusi√≥n.

En este caso, definimos los verdaderos positivos como aquellas instancias que forman parte de la misma partici√≥n, seg√∫n esta evaluaci√≥n externa, y tambi√©n forman parte de la misma partici√≥n en nuestro modelo de agrupamiento.

De forma similar, definimos los verdaderos negativos, falsos positivos y falsos negativos. A partir de aqu√≠, definimos el √≠ndice Rand como se muestra aqu√≠. B√°sicamente es los verdaderos positivos y negativos sobre el total de posibles pares de combinaciones.

- **√çndice Rand:** proporci√≥n de pares correctamente agrupados (0 = discordancia total, 1 = coincidencia perfecta).
$$
R = \frac{VP + VN}{VP + VN + FP + FN}  
$$
Eval√∫a la concordancia entre el agrupamiento generado y la clasificaci√≥n real.


---

# **Conclusiones**

- La **miner√≠a de datos** es un proceso sistem√°tico que combina t√©cnicas estad√≠sticas y de _machine learning_ para extraer conocimiento.
- La **metodolog√≠a CRISP-DM** proporciona una gu√≠a estructurada y c√≠clica.
- Es crucial entender la **tipolog√≠a del problema** (clasificaci√≥n, regresi√≥n, clustering) para elegir el algoritmo adecuado.
- La **evaluaci√≥n rigurosa** mediante m√©tricas apropiadas y particiones correctas garantiza la fiabilidad del modelo.

# **Glosario**
-  **aprendizaje autom√°tico o machine learning**: Conjunto de t√©cnicas, m√©todos y algoritmos que permiten a una m√°quina aprender de manera autom√°tica en base a experiencias pasadas.

- **aprendizaje no supervisado** Aprendizaje que se basa en el descubrimiento de patrones,
caracter√≠sticas y correlaciones en los datos de entrada, sin intervenci√≥n externa.

- **aprendizaje supervisado** Aprendizaje que se basa en el uso de un componente externo,
llamado supervisor, que compara los datos obtenidos por el modelo con los datos esperados
por este, y proporciona retroalimentaci√≥n al modelo para que vaya ajust√°ndose y mejorando
las predicciones.
# Recursos adicionales

-   Python Machine Learning: Learn How to Build Powerful Python Machine Learning Algorithms to Generate Useful Data Insights with This Data Analysis Tutorial
- Data mining and knowledge discovery (Online)
- Data Mining Algorithms: Explained Using R
- Introduction to machine learning with Python : a guide for data scientists / Andreas C. M√ºller and Sarah Guido.
- Mastering machine learning with scikit-learn : apply effective learning algorithms to real-world problems usung scikit-learn / Gavin Hackeling.
-   Advanced machine learning with Python : solve challenging data science problems by mastering cutting-edge machine learning techniques in Python / John Hearty.
- [Repositorio de la asignatura](https://gitlab.com/UOC/eimt/datascience/MUCD/AA)

---
---

# Preparaci√≥n de los datos

## **Introducci√≥n**

El m√≥dulo aborda las **t√©cnicas fundamentales de preprocesamiento de datos**, una fase esencial en la creaci√≥n de modelos de aprendizaje autom√°tico. Su prop√≥sito es **limpiar, transformar y adecuar los datos** para mejorar su calidad, eliminar ruido, corregir inconsistencias y asegurar su utilidad. La calidad de los datos determina el rendimiento del modelo (principio _GIGO: Garbage In, Garbage Out_).

---

## **Objetivos**

El texto busca que el lector:

1. Comprenda las t√©cnicas de **limpieza, discretizaci√≥n, normalizaci√≥n y estandarizaci√≥n**.
2. Conozca los m√©todos de **reducci√≥n de la dimensionalidad**.
3. Aprenda las t√©cnicas de **extracci√≥n y selecci√≥n de atributos**.
4. Entienda c√≥mo **tratar conjuntos desbalanceados** de datos.

---

## **1. Contextualizaci√≥n**

El preprocesamiento es clave para garantizar datos v√°lidos y consistentes.  

Se usa la notaci√≥n ( D_{n,m} ), donde:
- ( n ): n√∫mero de instancias.
- ( m ): n√∫mero de atributos.
- ( a_{i,j} ): valor del atributo ( j ) de la instancia ( i ).
- ( c_i ): clase o etiqueta (solo en aprendizaje supervisado).    

Los datos sin etiqueta (no supervisados) mantienen la misma estructura pero sin ( c_i ).

---

## **2. Conceptos preliminares**

### **2.1. An√°lisis estad√≠stico**

Introduce los fundamentos de la estad√≠stica aplicados a la miner√≠a de datos: distribuci√≥n de una muestra, media, mediana, varianza, desviaci√≥n est√°ndar y distribuci√≥n normal.  

Se explica c√≥mo la **distribuci√≥n normal est√°ndar (Z)** permite comparar valores distintos mediante la conversi√≥n ( $Z = (X - \mu)/\sigma$ ).

### **2.2. M√©tricas de distancias o similitud**

Define las propiedades de una funci√≥n de distancia (no negatividad, simetr√≠a y desigualdad triangular).  

Ejemplos:
- **Eucl√≠dea**: m√°s usada en espacios num√©ricos.
- **Manhattan**, **Chebyshev**, **Minkowski** y **coseno**: aplicables seg√∫n el tipo de datos.  

 La elecci√≥n de la m√©trica es crucial para los algoritmos de segmentaci√≥n y clasificaci√≥n.

### **2.3. Entrop√≠a y ganancia de informaci√≥n**

La **entrop√≠a** mide el grado de desorden o incertidumbre ($H(X) = -\sum p(x_i) \log_2 p(x_i)$).  

La **ganancia de informaci√≥n (IG)** eval√∫a cu√°nto reduce un atributo la entrop√≠a total:  
 $IG(X,a) = H(X) - H(X|a)$.  
 
Un atributo con alta IG es m√°s relevante para la predicci√≥n.

---

## **3. Preparaci√≥n de los datos**

### **3.1. Limpieza de datos**

Detecta y corrige errores, valores ausentes, inconsistentes o at√≠picos (_outliers_).  

La integraci√≥n de m√∫ltiples fuentes puede generar incoherencias que deben resolverse.
### **3.2. Normalizaci√≥n y estandarizaci√≥n**

Ambas transforman variables num√©ricas para mejorar la precisi√≥n de los modelos:

- **Normalizaci√≥n**: ajusta valores a un rango fijo (ej. [0,1]).
- **Estandarizaci√≥n**: centra los datos en media 0 y varianza 1.  
    
Evitan que una variable con gran escala domine sobre las dem√°s.
### **3.3. Discretizaci√≥n**

Convierte variables continuas en categ√≥ricas.  

M√©todos:
- **Basados en intervalos**: igual amplitud o igual frecuencia.
- **Chi-Merge**: usa el estad√≠stico œá¬≤ para unir o dividir intervalos.
- **Basados en entrop√≠a**: buscan particiones homog√©neas minimizando la entrop√≠a interna.

### **3.4. Reducci√≥n de la dimensionalidad**

Permite manejar datasets grandes reduciendo columnas (atributos) o filas (casos).

- **Reducci√≥n de atributos**:
    - **Selecci√≥n de atributos**: identifica los m√°s relevantes (por significaci√≥n o correlaci√≥n).
    - **Creaci√≥n de atributos**: combina variables (p. ej., **PCA**: an√°lisis de componentes principales).
- **Reducci√≥n de casos**: se basa en muestras representativas y distribuciones muestrales.

---

## **4. Extracci√≥n y selecci√≥n de atributos**

### **4.1. Selecci√≥n de atributos**

Busca eliminar variables redundantes o irrelevantes para reducir la complejidad.  

Se emplean:
- **M√©todos filtro**: usan medidas estad√≠sticas (correlaci√≥n, informaci√≥n mutua).
- **M√©todos envolventes (wrapper)**: eval√∫an subconjuntos con modelos predictivos.
- **M√©todos integrados**: el modelo selecciona atributos autom√°ticamente (ej. LASSO).

### **4.2. Extracci√≥n de atributos**

Crea nuevas caracter√≠sticas a partir de las existentes:

- **PCA (An√°lisis de Componentes Principales)**: combina variables para maximizar varianza y reducir error cuadr√°tico.
- **SVD (Descomposici√≥n en Valores Singulares)**: descompone matrices para representar la informaci√≥n en menos dimensiones.
- **NMF (Factorizaci√≥n de Matrices No Negativas)**: descompone datos no negativos en factores interpretables.

---

## **5. Conjuntos desbalanceados de datos**

### **5.1. Definici√≥n y problemas**

Ocurre cuando una clase tiene muchas m√°s instancias que otra (ej. fraude bancario 0.1%).  

Consecuencias:
- Sesgo hacia clases mayoritarias.
- Baja precisi√≥n/sensibilidad en clases minoritarias.
- Riesgo de **sobreajuste** y **evaluaciones enga√±osas** (la exactitud puede parecer alta sin serlo).

### **5.2. T√©cnicas de sobremuestreo**

Aumentan el n√∫mero de ejemplos de la clase minoritaria:
- **Duplicaci√≥n aleatoria** o **s√≠ntesis artificial (SMOTE)**.

### **5.3. T√©cnicas de submuestreo**

Reducen el n√∫mero de ejemplos de la clase mayoritaria:
- **Eliminaci√≥n aleatoria** o selecci√≥n basada en distancias.

### **5.4. M√©tricas de evaluaci√≥n**

Cuando los datos est√°n desbalanceados, se prefieren m√©tricas como:
- **Precisi√≥n**, **sensibilidad (recall)**, **F1-Score**, y **ROC-AUC**.  
    Se recomienda **validaci√≥n cruzada estratificada** para conservar la proporci√≥n de clases.

---

## **Resumen final**

El m√≥dulo presenta una visi√≥n integral del **preprocesamiento de datos**: desde la comprensi√≥n estad√≠stica y las m√©tricas de similitud hasta la limpieza, normalizaci√≥n, discretizaci√≥n y reducci√≥n de la dimensionalidad.  

Tambi√©n aborda la **selecci√≥n/extracci√≥n de atributos** y las **estrategias para manejar conjuntos desbalanceados**, destacando su impacto directo en la calidad y fiabilidad de los modelos de aprendizaje autom√°tico.
